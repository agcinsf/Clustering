{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LS COE Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this analysis is to group a set of items into categories using their descriptions.  Conceptually, we will put all items with \"similar\" descriptions into the same group.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_data = pd.read_csv('c:/users/aclark/Desktop/My Box Files/LifeScience_COE_DataCollection/Clustering/lscoeItems.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##################### Vectorize the inputs #############################\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='char_wb', lowercase=True, ngram_range=(2,2))\n",
    "Vectorized_Descr = tfidf.fit_transform(raw_data['cleandescr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Use Mini-Batch KMeans to cluster into 20 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I dont expect this to work well, it should be fast and provide an answer.  It will serve as the base line for other tests and methods.  The input data is scrubbed using the following regexp (in postgres):\n",
    "\n",
    "regexp_replace(regexp_replace(regexp_replace(lscoe.\"Item Description\", '[\\t?&\"\\\\/)(\\*\\]\\[]','','g'), '\\s+',' ','g'),'^\\s+','','g')\n",
    "\n",
    "See the process document in the clustering folder for details and the SQL Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "####################\n",
    "\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=10, compute_labels=True)\n",
    "km = mbk.fit(Vectorized_Descr)\n",
    "\n",
    "\n",
    "################  Writeout results ##############################\n",
    "\n",
    "#Convert to dataframe and pair up with the origional data\n",
    "labels = pd.DataFrame(km.predict(Vectorized_Descr))\n",
    "labels.columns = ['label']\n",
    "                      \n",
    "outframe = pd.merge(raw_data,labels, right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "#write out the csv file\n",
    "outframe.to_csv('c:/users/aclark/Desktop/My Box Files/LifeScience_COE_DataCollection/Clustering/mbk.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: Spectral Clustering using Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method doesnt work because of the size of the data.  Spectral Clustering a 108k x 108k matrix seems bigger than 10GB of Ram can manage.  Will try and modify the query to pull only the top 80% of spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e8c1a55afe01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Calculate the Cosine Distance Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mCosMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mVectorized_Descr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mVectorized_Descr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Unsupervised clustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.pyc\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mnnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mdata\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsetools\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_matmat_pass2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "#Calculate the Cosine Distance Matrix\n",
    "CosMatrix = (Vectorized_Descr * Vectorized_Descr.T)\n",
    "        \n",
    "#Unsupervised clustering \n",
    "spec = SpectralClustering(n_clusters=20)\n",
    "Cluster =spec.fit(CosMatrix)\n",
    "\n",
    "#Append the cluster labels to the origional threads\n",
    "temp['Labels'] = Cluster.labels_\n",
    "    \n",
    "temp.to_csv('C:/Users/aclark/Desktop/My Box Files/LifeScience_COE_DataCollection/Clustering/spectral.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3:  Linear SVM using Greg's Data From 2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using greg's classifications from the 2008 GLS RFP, I will reformulate this problem as a Supervised Learning problem.  As a result, I can train a Linear SVM on Greg's data and use it to cluster the COE data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized data\n",
      "Starting Classification\n",
      "0.760680529301\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Import gregs data, a 2 dimensional array with labels in the first column and descriptions in the second.\n",
    "SVMData = pd.read_csv('c:/users/aclark/Desktop/My Box Files/LifeScience_COE_DataCollection/Clustering/TopItems.csv')\n",
    "\n",
    "#Vectorize the entire population of data \n",
    "#tfidf = TfidfVectorizer(analyzer='word', lowercase=True, ngram_range=(1,1), decode_error='ignore', max_df=.3)\n",
    "tfidf = CountVectorizer(analyzer='char_wb', binary=True, max_df=.2, decode_error='ignore', ngram_range=(4,4))\n",
    "\n",
    "Vectorized_Descr = tfidf.fit_transform(SVMData['cleandescr'])\n",
    "\n",
    "\n",
    "print('vectorized data')\n",
    "\n",
    "#Split out the Gregs Data from the COE Data\n",
    "gX = Vectorized_Descr[0:8137]\n",
    "gy = SVMData['label'][0:8137]\n",
    "\n",
    "# Take a portion of the vectorized descriptions as a training set another as testing set.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(gX, gy ,test_size=.65, random_state=10)\n",
    "\n",
    "print('Starting Classification')\n",
    "\n",
    "svc = LinearSVC(C=10000, class_weight='auto', dual=False, fit_intercept=False)\n",
    "svc.fit(X_train,Y_train)\n",
    "\n",
    "scores = svc.score(X_test,Y_test)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6102x121501 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 120015 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing data\n",
      "Starting Classification\n",
      "Scoring Results\n",
      "training accuracy\n",
      "0.996108152397\n",
      "Writing Results\n",
      "0.720430107527\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Import gregs data, a 2 dimensional array with labels in the first column and descriptions in the second.\n",
    "RFCData = pd.read_csv('c:/users/aclark/Desktop/My Box Files/LifeScience_COE_DataCollection/Clustering/TopItems.csv')\n",
    "\n",
    "print('vectorizing data')\n",
    "#Vectorize the entire population of data \n",
    "tfidf = TfidfVectorizer(analyzer='word', lowercase=True, ngram_range=(1,1), max_df=.2, decode_error='ignore')\n",
    "Vectorized_Descr = tfidf.fit_transform(RFCData['cleandescr'])\n",
    "\n",
    "#Split out the Gregs Data from the COE Data\n",
    "gX = Vectorized_Descr[0:8137]\n",
    "gy = RFCData['label'][0:8137]\n",
    "\n",
    "# Take a portion of the vectorized descriptions as a training set another as testing set.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(gX, gy ,test_size=.40, random_state=10)\n",
    "\n",
    "print('Starting Classification')\n",
    "RFC = RandomForestClassifier(n_estimators=25, n_jobs=1)\n",
    "RFC.fit(X_train.toarray(),Y_train)\n",
    "\n",
    "print('Scoring Results')\n",
    "scores = RFC.score(X_test.toarray(),Y_test)\n",
    "labels = pd.DataFrame(RFC.predict(Vectorized_Descr[8137:].toarray()))\n",
    "\n",
    "print('training accuracy')\n",
    "print(RFC.score(X_train.toarray(),Y_train))\n",
    "\n",
    "\n",
    "print('Writing Results')\n",
    "outframe = pd.merge(RFCData[8137:].reindex(),labels, right_index=True, left_index=True)\n",
    "\n",
    "outframe.to_csv('c:/users/aclark/Desktop/My Box Files/LifeScience_COE_DataCollection/Clustering/RFCResults.csv')\n",
    "\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
